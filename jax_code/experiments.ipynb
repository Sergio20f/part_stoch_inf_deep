{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import arch, brax\n",
    "from datasets import get_dataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, vmap\n",
    "from jax.example_libraries import optimizers, stax\n",
    "from jax.tree_util import tree_map\n",
    "from utils import get_calibration, jaxRNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10C(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, severity_level=1):\n",
    "        # Validate severity level\n",
    "        if severity_level not in range(1, 6):\n",
    "            raise ValueError(\"Severity level must be between 1 and 5.\")\n",
    "            \n",
    "        self.data_files = sorted([f for f in Path(data_dir).glob('*.npy') if 'labels.npy' not in str(f)])\n",
    "        self.labels = np.load(Path(data_dir) / 'labels.npy')\n",
    "        self.transform = transform\n",
    "        self.severity_level = severity_level\n",
    "        # Each severity level has 10000 images\n",
    "        self.images_per_level = 10000\n",
    "        self.start_idx = self.images_per_level * (severity_level - 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Assuming same number of images per severity level\n",
    "        return self.images_per_level\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Adjust idx based on severity level\n",
    "        adjusted_idx = self.start_idx + idx\n",
    "        # Assuming data_files are in the correct order\n",
    "        data = np.load(self.data_files[adjusted_idx // self.images_per_level])[adjusted_idx % self.images_per_level]\n",
    "        label = self.labels[adjusted_idx % self.images_per_level]\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading checkpoints...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: sdebnn\n",
      "Using CIFAR-10-C dataset at severity level 3\n",
      "Number of test batches: 1\n",
      "Successfully loaded checkpoints for epoch 95 with best validation accuracy 0.7594000101089478\n"
     ]
    }
   ],
   "source": [
    "# Define all the args\n",
    "model= 'sdebnn' #'psdebnn'\n",
    "print(\"Using model:\", model)\n",
    "output='output-sdebnn'\n",
    "seed=0\n",
    "stl=False\n",
    "lr=0.0007\n",
    "epochs=100\n",
    "bs=128\n",
    "test_bs=1000\n",
    "nsamples=1\n",
    "w_init=-1.0\n",
    "b_init=-1.0\n",
    "p_init=-1.0\n",
    "pause_every=200\n",
    "no_drift=False\n",
    "ou_dw=True\n",
    "kl_coef=0.001\n",
    "diff_coef=0.1\n",
    "ds='cifar10'\n",
    "severity_level = 3 # CHANGE ACCORDINGLY\n",
    "no_xt=True\n",
    "acc_grad=1\n",
    "aug=0\n",
    "remat=False\n",
    "ema=0.999\n",
    "meanfield_sdebnn=False\n",
    "infer_w0=False\n",
    "w0_prior_std=0.1\n",
    "disable_test=False\n",
    "verbose=True\n",
    "nblocks='2-2-2'\n",
    "block_type=0\n",
    "fx_dim=64\n",
    "fx_actfn='softplus'\n",
    "fw_dims='2-128-2'\n",
    "fw_actfn='softplus'\n",
    "lr_sched='constant'\n",
    "rng_generator = jaxRNG(seed= seed)\n",
    "\n",
    "ode_first=True\n",
    "timecut=0.1\n",
    "method_ode='euler'\n",
    "\n",
    "if ds == 'cifar10':\n",
    "    print(\"Using CIFAR-10 dataset\")\n",
    "    train_loader, train_eval_loader, val_loader, test_loader, input_size, train_size = get_dataset(bs, test_bs, \"cifar10\")\n",
    "    num_batches = len(test_loader)\n",
    "    print(f\"Number of test batches: {num_batches}\")\n",
    "\n",
    "elif ds == 'cifar10c':\n",
    "    print(\"Using CIFAR-10-C dataset at severity level\", severity_level)\n",
    "    # Need to download manually from https://zenodo.org/records/2535967/files/CIFAR-10-C.tar?download=1\n",
    "    data_dir = \"data/cifar10c\"\n",
    "\n",
    "    # Define your transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    cifar10c_dataset = CIFAR10C(data_dir=data_dir, transform=transform, severity_level=severity_level)\n",
    "\n",
    "    # Create test DataLoader for the CIFAR-10-C dataset\n",
    "    #test_loader = DataLoader(cifar10c_dataset, batch_size=test_bs, shuffle=True)\n",
    "    #num_batches = len(test_loader)\n",
    "    #print(f\"Number of test batches: {num_batches}\")\n",
    "\n",
    "    # Since DataLoader will iterate over the dataset, we can just grab the first 1000 images\n",
    "    # during the actual testing loop. If we want to explicitly create a smaller dataset:\n",
    "\n",
    "    indices = torch.randperm(len(cifar10c_dataset)).tolist()\n",
    "    test_subset_indices = indices[:1000]  # Get the first 1000 indices after shuffling\n",
    "    test_subset = torch.utils.data.Subset(cifar10c_dataset, test_subset_indices)\n",
    "    test_loader = DataLoader(test_subset, batch_size=test_bs, shuffle=False)\n",
    "    num_batches = len(test_loader)\n",
    "    print(f\"Number of test batches: {num_batches}\")\n",
    "else:\n",
    "    raise ValueError(\"Dataset not supported!\")\n",
    "    \n",
    "# SDEBNN specific\n",
    "mf = partial(brax.MeanField, disable=True) if kl_coef == 0. else brax.MeanField\n",
    "fw_dims = list(map(int, fw_dims.split(\"-\")))\n",
    "layers = [mf(arch.Augment( aug))]\n",
    "nblocks = list(map(int, nblocks.split(\"-\")))\n",
    "opt_init, opt_update, get_params = optimizers.adam(7e-4)\n",
    "\n",
    "# Load the checkpoint if it exists\n",
    "checkpoint_path = os.path.join(output, 'best_model_checkpoint.pkl')\n",
    "if os.path.exists(checkpoint_path):\n",
    "    logging.warning(\"Loading checkpoints...\")\n",
    "    with open(checkpoint_path, \"rb\") as f:\n",
    "        checkpoint = pickle.load(f)\n",
    "\n",
    "    # Extract states from the checkpoint\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_val_acc = checkpoint['best_val_acc']\n",
    "    global_step = checkpoint['global_step']\n",
    "    opt_state = checkpoint['optimizer_state']\n",
    "    ema_params = checkpoint['ema_state']\n",
    "    params = checkpoint['model_state']  # Loaded parameters\n",
    "    print(f\"Successfully loaded checkpoints for epoch {start_epoch} with best validation accuracy {best_val_acc}\")\n",
    "else:\n",
    "    raise SystemExit(\"No checkpoint found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment\n",
      "there were [  0.   0.   2.  30.  66.  82.  84.  85. 130. 520.] items in bins 1-10\n",
      "Inference time with 30 steps: 38.07104778289795 (s)\n",
      "Accuracy with 30 steps: 0.7870000600814819\n",
      "NLL with 30 steps: 0.0006241790251806378\n",
      "ECE with 30 steps: 0.0401261042443063\n",
      "Experiment run sucessfully!\n"
     ]
    }
   ],
   "source": [
    "def _nll(params, batch, rng):\n",
    "    inputs, targets = batch\n",
    "    preds, kl, info_dict = _predict(params, inputs, rng=rng, full_output=False)\n",
    "    nll = -jnp.mean(jnp.sum(preds * targets, axis=1))\n",
    "    return preds, nll, kl, info_dict\n",
    "\n",
    "\n",
    "@partial(jit, static_argnums=(3,))\n",
    "def sep_loss(params, batch, rng, kl_coef):  # no backprop\n",
    "    preds, nll, kl, _ = _nll(params, batch, rng)\n",
    "    if kl_coef > 0:\n",
    "        obj_loss = nll + kl * kl_coef\n",
    "    else:\n",
    "        obj_loss = nll\n",
    "    _sep_loss = {'loss': obj_loss, 'kl': kl, 'nll': nll, 'preds': preds}\n",
    "    return obj_loss, _sep_loss\n",
    "\n",
    "@partial(jit, static_argnums=(3,))\n",
    "def loss(params, batch, rng, kl_coef):  # backprop so checkpoint\n",
    "    _, nll, kl, _ = jax.checkpoint(_nll)(params, batch, rng)\n",
    "    if kl_coef > 0:\n",
    "        return nll + kl * kl_coef\n",
    "    else:\n",
    "        return nll\n",
    "\n",
    "@jit\n",
    "def predict(params, inputs, rng): \n",
    "    return _predict(params, inputs, rng=rng, full_output=True)\n",
    "\n",
    "@partial(jit, static_argnums=(2,))\n",
    "def accuracy(params, data, nsamples, rng):\n",
    "    inputs, targets = data\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    rngs = jax.random.split(rng, nsamples)\n",
    "    preds, _, info_dic = vmap(predict, in_axes=(None, None, 0))(params, inputs, rngs)\n",
    "    preds = jnp.stack(preds, axis=0)\n",
    "    avg_preds = preds.mean(0)\n",
    "    predicted_class = jnp.argmax(avg_preds, axis=1)\n",
    "    n_correct = jnp.sum(predicted_class == target_class)\n",
    "    n_total = inputs.shape[0]\n",
    "    wts = info_dic[model+'_w'] \n",
    "    wts = jnp.stack(wts, axis=0)\n",
    "    avg_wts = wts.mean(0)\n",
    "    return n_correct, n_total, avg_preds, avg_wts\n",
    "\n",
    "def update_ema(ema_params, params, momentum=0.999):\n",
    "    return tree_map(lambda e, p: e * momentum + p * (1 - momentum), ema_params, params)\n",
    "\n",
    "def evaluate(params, data_loader, input_size, nsamples, rng_generator, kl_coef):\n",
    "    n_total = 0\n",
    "    n_correct = 0\n",
    "    nll = 0\n",
    "    kl = 0\n",
    "    logits = np.array([])\n",
    "    wts = np.array([])\n",
    "    labels = np.array([])\n",
    "    for inputs, targets in data_loader:\n",
    "        targets = jax.nn.one_hot(jnp.array(targets), num_classes=10)\n",
    "        inputs = jnp.array(inputs).reshape((-1,) + (input_size[-1],) + input_size[:2])\n",
    "        inputs = jnp.transpose(inputs, (0, 2, 3, 1))  # Permute from NCHW to NHWC\n",
    "        batch_correct, batch_total, _logits, _wts = accuracy(\n",
    "            params, (inputs, targets), nsamples, rng_generator.next()\n",
    "        )\n",
    "        n_correct = n_correct + batch_correct\n",
    "        _, batch_nll, batch_kl, _ = jit(_nll)(params, (inputs, targets), rng_generator.next())\n",
    "        if n_total == 0:\n",
    "            logits = np.array(_logits)\n",
    "            wts = np.array(_wts)\n",
    "            labels = np.array(targets)\n",
    "        else:\n",
    "            logits = np.concatenate([logits, np.array(_logits)], axis=0)\n",
    "            wts = np.concatenate([wts, np.array(_wts)], axis=0)\n",
    "            labels = np.concatenate([labels, targets], axis=0)\n",
    "        n_total = n_total + batch_total\n",
    "        nll = nll + batch_nll\n",
    "        kl = kl + batch_kl\n",
    "    return n_correct / n_total, jnp.stack(logits, axis=0), labels, nll / n_total, kl / n_total, jnp.stack(wts, axis=0)\n",
    "\n",
    "nsteps = 30 # CHANGE FOR EACH EXPERIMENT\n",
    "print(\"Starting experiment\")\n",
    "\n",
    "if model == \"sdebnn\":\n",
    "    for i, nb in enumerate(nblocks):\n",
    "        fw = arch.MLP(fw_dims, actfn=fw_actfn, xt=no_xt, ou_dw=ou_dw, nonzero_w=w_init, nonzero_b=b_init, p_scale=p_init)  # weight network is time dependent\n",
    "        if meanfield_sdebnn:\n",
    "            layers.extend([mf(brax.SDEBNN(block_type,\n",
    "                                            fx_dim,\n",
    "                                            fx_actfn,\n",
    "                                            fw,\n",
    "                                            diff_coef=diff_coef,\n",
    "                                            stl=stl,\n",
    "                                            xt=no_xt,\n",
    "                                            nsteps=nsteps,\n",
    "                                            remat=remat,\n",
    "                                            w_drift=not no_drift,\n",
    "                                            stax_api=True,\n",
    "                                            infer_initial_state=infer_w0,\n",
    "                                            initial_state_prior_std=w0_prior_std)) for _ in range(nb)\n",
    "            ])\n",
    "        else:\n",
    "            layers.extend([brax.SDEBNN( block_type,\n",
    "                                        fx_dim,\n",
    "                                        fx_actfn,\n",
    "                                        fw,\n",
    "                                        diff_coef=diff_coef,\n",
    "                                        stl=stl,\n",
    "                                        xt=no_xt,\n",
    "                                        nsteps=nsteps,\n",
    "                                        remat=remat,\n",
    "                                        w_drift=not no_drift,\n",
    "                                        infer_initial_state=infer_w0,\n",
    "                                        initial_state_prior_std=w0_prior_std) for _ in range(nb)\n",
    "            ])\n",
    "        if i < len(nblocks) - 1:\n",
    "            layers.append(mf(arch.SqueezeDownsample(2)))\n",
    "    layers.append(mf(stax.serial(stax.Flatten, stax.Dense(10), stax.LogSoftmax)))\n",
    "\n",
    "    init_random_params, _predict = brax.bnn_serial(*layers)\n",
    "\n",
    "elif model == \"psdebnn\":\n",
    "    for i, nb in enumerate(nblocks):\n",
    "        fw = arch.MLP(fw_dims, actfn=fw_actfn, xt=no_xt, ou_dw=ou_dw, nonzero_w=w_init, nonzero_b=b_init, p_scale=p_init)  # weight network is time dependent\n",
    "        if meanfield_sdebnn:\n",
    "            layers.extend([mf(brax.PSDEBNN( block_type,\n",
    "                                            fx_dim,\n",
    "                                            fx_actfn,\n",
    "                                            fw,\n",
    "                                            diff_coef=diff_coef,\n",
    "                                            stl=stl,\n",
    "                                            xt=no_xt,\n",
    "                                            nsteps=nsteps,\n",
    "                                            remat=remat,\n",
    "                                            w_drift=not no_drift,\n",
    "                                            stax_api=True,\n",
    "                                            infer_initial_state=infer_w0,\n",
    "                                            initial_state_prior_std=w0_prior_std,\n",
    "                                            ode_first=ode_first,\n",
    "                                            timecut=timecut,\n",
    "                                            method_ode=method_ode)) for _ in range(nb)\n",
    "            ])\n",
    "        else:\n",
    "            layers.extend([brax.PSDEBNN(block_type,\n",
    "                                        fx_dim,\n",
    "                                        fx_actfn,\n",
    "                                        fw,\n",
    "                                        diff_coef=diff_coef,\n",
    "                                        stl=stl,\n",
    "                                        xt=no_xt,\n",
    "                                        nsteps=nsteps,\n",
    "                                        remat=remat,\n",
    "                                        w_drift=not no_drift,\n",
    "                                        infer_initial_state=infer_w0,\n",
    "                                        initial_state_prior_std=w0_prior_std,\n",
    "                                        ode_first=ode_first,\n",
    "                                        timecut=timecut,\n",
    "                                        method_ode=method_ode) for _ in range(nb)\n",
    "            ])\n",
    "        if i < len(nblocks) - 1:\n",
    "            layers.append(mf(arch.SqueezeDownsample(2)))\n",
    "    layers.append(mf(stax.serial(stax.Flatten, stax.Dense(10), stax.LogSoftmax)))\n",
    "\n",
    "    init_random_params, _predict = brax.bnn_serial(*layers)\n",
    "\n",
    "# for inputs, targets in tqdm(test_loader): # evaluate already deals with the loop\n",
    "start_time = time.time()\n",
    "acc, logits, targets, nll, _, _ = evaluate(params, test_loader, input_size, nsamples, rng_generator, kl_coef=kl_coef)\n",
    "\n",
    "# Calculate inference time\n",
    "inference_time = time.time() - start_time\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probabilities = jax.nn.softmax(logits)\n",
    "    \n",
    "# Calculate ECE\n",
    "# TODO: might be better to use utils.ECE or utils.compute_acc_bin\n",
    "cal = get_calibration(targets, probabilities)\n",
    "\n",
    "print(f\"Inference time with {nsteps} steps: {inference_time} (s)\")\n",
    "print(f\"Accuracy with {nsteps} steps: {acc}\")\n",
    "print(f\"NLL with {nsteps} steps: {nll}\")\n",
    "print(f\"ECE with {nsteps} steps: {cal['ece']}\")\n",
    "\n",
    "print(\"Experiment run sucessfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxsde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
