{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Wrote config: train/config.json\n",
      "WARNING:root:Creating loaders for data mnist for task classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:train before split: 60000\n",
      "WARNING:root:train: 54000 | val: 6000 | test: 10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "True 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Training set size: 53888, Val set size: 6000, test set size: 10000\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import copy\n",
    "import gc\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import torchsde\n",
    "import numpy as np\n",
    "\n",
    "import models\n",
    "import utils\n",
    "import tqdm\n",
    "import diffeq_layers\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "def make_y_net(input_size,\n",
    "               blocks=(2, 2, 2),\n",
    "               activation=\"softplus\",\n",
    "               verbose=False,\n",
    "               explicit_params=True,\n",
    "               hidden_width=128,\n",
    "               aug_dim=0):\n",
    "    \n",
    "    \"\"\"This is the bayesian neural network\"\"\"\n",
    "\n",
    "    _input_size = (input_size[0] + aug_dim,) + input_size[1:]\n",
    "    layers = []\n",
    "    print(f\"input_size ynet: {_input_size}\")\n",
    "\n",
    "    for i, num_blocks in enumerate(blocks, 1):\n",
    "        for j in range(1, num_blocks + 1):\n",
    "\n",
    "            print(_input_size, 'forloop')\n",
    "            layers.extend(diffeq_layers.make_ode_k3_block_layers(input_size=_input_size,\n",
    "                                                                 activation=activation,\n",
    "                                                                 last_activation=i < len(blocks) or j < num_blocks,\n",
    "                                                                 hidden_width=hidden_width, mode=1))\n",
    "\n",
    "            if verbose:\n",
    "                if i == 1:\n",
    "                    print(f\"y_net (augmented) input size: {_input_size}\")\n",
    "                layers.append(diffeq_layers.Print(name=f\"group: {i}, block: {j}\"))\n",
    "\n",
    "        if i < len(blocks):\n",
    "            layers.append(diffeq_layers.ConvDownsample(_input_size))\n",
    "            _input_size = _input_size[0] * 4, _input_size[1] // 2, _input_size[2] // 2\n",
    "            print(_input_size, 'convdownsample')\n",
    "\n",
    "    y_net = diffeq_layers.DiffEqSequential(*layers, explicit_params=explicit_params)\n",
    "\n",
    "    # return augmented input size b/c y net should have same input / output\n",
    "    return y_net, _input_size\n",
    "\n",
    "\n",
    "# TODO: add STL\n",
    "class SDENet(torchsde.SDEStratonovich):\n",
    "    def __init__(self,\n",
    "                 input_size=(3, 32, 32),\n",
    "                 blocks=(2, 2, 2),\n",
    "                 weight_network_sizes=(1, 64, 1),\n",
    "                 num_classes=10,\n",
    "                 activation=\"softplus\",\n",
    "                 verbose=False,\n",
    "                 inhomogeneous=True,\n",
    "                 sigma=0.1,\n",
    "                 hidden_width=128,\n",
    "                 aug_dim=0):\n",
    "        super(SDENet, self).__init__(noise_type=\"diagonal\")\n",
    "        self.input_size = input_size\n",
    "        self.aug_input_size = (aug_dim + input_size[0], *input_size[1:])\n",
    "        self.aug_zeros_size = (aug_dim, *input_size[1:])\n",
    "        self.register_buffer('aug_zeros', torch.zeros(size=(1, *self.aug_zeros_size)))\n",
    "\n",
    "        # Create network evolving state.\n",
    "        self.y_net, self.output_size = make_y_net(\n",
    "            input_size=input_size,\n",
    "            blocks=blocks,\n",
    "            activation=activation,\n",
    "            verbose=verbose,\n",
    "            hidden_width=hidden_width,\n",
    "            aug_dim=aug_dim\n",
    "        )\n",
    "        # Create network evolving weights.\n",
    "        initial_params = self.y_net.make_initial_params()  # w0.\n",
    "        flat_initial_params, unravel_params = utils.ravel_pytree(initial_params)\n",
    "        self.flat_initial_params = nn.Parameter(flat_initial_params, requires_grad=True)\n",
    "        self.params_size = flat_initial_params.numel()\n",
    "        print(f\"initial_params ({self.params_size}): {flat_initial_params.shape}\")\n",
    "        self.unravel_params = unravel_params\n",
    "        self.w_net = models.make_w_net(\n",
    "            in_features=self.params_size,\n",
    "            hidden_sizes=weight_network_sizes,\n",
    "            activation=\"tanh\",\n",
    "            inhomogeneous=inhomogeneous\n",
    "        )\n",
    "\n",
    "        # Final projection layer.\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # nn.Linear(int(np.prod(self.output_size)), num_classes), # option: projection w/o ReLU\n",
    "            nn.Linear(int(np.prod(self.output_size)), 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, num_classes),\n",
    "        )\n",
    "\n",
    "        self.register_buffer('ts', torch.tensor([0., 1.]))\n",
    "        self.sigma = sigma\n",
    "        self.nfe = 0\n",
    "\n",
    "    def f(self, t, y: torch.Tensor):\n",
    "        input_y = y\n",
    "        self.nfe += 1\n",
    "        y, w, _ = y.split(split_size=(y.numel() - self.params_size - 1, self.params_size, 1), dim=1) # params_size: 606408\n",
    "\n",
    "        fy = self.y_net(t, y.reshape((-1, *self.aug_input_size)), self.unravel_params(w.squeeze(0))).reshape(-1).unsqueeze(0)\n",
    "        nn = self.w_net(t, w)\n",
    "        fw = nn - w  # hardcoded OU prior on weights w\n",
    "        fl = (nn ** 2).sum(dim=1, keepdim=True) / (self.sigma ** 2)\n",
    "\n",
    "        assert input_y.shape == torch.cat([fy, fw, fl], dim=1).shape, f\"Want: {input_y.shape} Got: {torch.cat((fy, fw, fl)).shape}. Check nblocks for dataset divisibility.\\n\"\n",
    "        return torch.cat([fy, fw, fl], dim=1)#.squeeze(0)\n",
    "\n",
    "    def g(self, t, y):\n",
    "        self.nfe += 1\n",
    "        gy = torch.zeros(size=(y.numel() - self.params_size - 1,), device=y.device)\n",
    "        gw = torch.full(size=(self.params_size,), fill_value=self.sigma, device=y.device)\n",
    "        gl = torch.tensor([0.], device=y.device)\n",
    "        return torch.cat([gy, gw, gl], dim=0).unsqueeze(0)\n",
    "\n",
    "    def make_initial_params(self):\n",
    "        return self.y_net.make_initial_params()\n",
    "\n",
    "    def forward(self, y, adjoint=False, dt=0.02, adaptive=False, adjoint_adaptive=False, method=\"midpoint\", rtol=1e-4, atol=1e-3):\n",
    "        # Note: This works correctly, as long as we are requesting the nfe after each gradient update.\n",
    "        #  There are obviously cleaner ways to achieve this.\n",
    "        self.nfe = 0    \n",
    "        sdeint = torchsde.sdeint_adjoint if adjoint else torchsde.sdeint\n",
    "        if self.aug_zeros.numel() > 0:  # Add zero channels.\n",
    "            aug_zeros = self.aug_zeros.expand(y.shape[0], *self.aug_zeros_size)\n",
    "            y = torch.cat([y, aug_zeros], dim=1) # 235200\n",
    "        aug_y = torch.cat((y.reshape(-1), self.flat_initial_params, torch.tensor([0.], device=y.device))) # 841609: (235200, 606408, 1)\n",
    "        aug_y = aug_y[None]\n",
    "        bm = torchsde.BrownianInterval(\n",
    "            t0=self.ts[0], t1=self.ts[-1], size=aug_y.shape, dtype=aug_y.dtype, device=aug_y.device,\n",
    "            cache_size=45 if adjoint else 30  # If not adjoint, don't really need to cache.\n",
    "        )\n",
    "        if adjoint_adaptive:\n",
    "            _, aug_y1 = sdeint(self, aug_y, self.ts, bm=bm, method=method, dt=dt, adaptive=adaptive, adjoint_adaptive=adjoint_adaptive, rtol=rtol, atol=atol)\n",
    "        else:\n",
    "            _, aug_y1 = sdeint(self, aug_y, self.ts, bm=bm, method=method, dt=dt, adaptive=adaptive, rtol=rtol, atol=atol)\n",
    "        \n",
    "        print(aug_y1.shape, 'ww, aug_y1')\n",
    "\n",
    "        y1 = aug_y1[:,:y.numel()].reshape(y.size())\n",
    "        logits = self.projection(y1)\n",
    "        #logits = nn.functional.softmax(logits, dim=1)\n",
    "        logqp = .5 * aug_y1[-1]\n",
    "        return logits, logqp\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        for p in self.parameters(): p.grad = None\n",
    "\n",
    "print(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "def train(model, ema_model, optimizer, scheduler, epochs, global_step=0, output_dir=None, start_epoch=0,\n",
    "          best_test=0, best_val=0, info=collections.defaultdict(dict), tb_writer=None):\n",
    "    \n",
    "    def add_scalar_(name: str, arg1, arg2):\n",
    "        if torch.is_tensor(arg1):\n",
    "            tb_writer.add_scalar(name, arg1[0], arg2)\n",
    "        else:\n",
    "            tb_writer.add_scalar(name, arg1, arg2)\n",
    "    \n",
    "    train_xent = utils.EMAMeter()\n",
    "    train_accuracy = utils.EMAMeter()\n",
    "    test_accuracy, best_test_acc = 0, best_test\n",
    "    val_xent, val_xent_ema, val_accuracy, best_val_acc = 0, 0, 0, best_val\n",
    "    obj, kl, ece, nfe = 0, 0, utils.AverageMeter(), utils.AverageMeter()\n",
    "    epoch_start = time.time()\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        itr_per_epoch = 0\n",
    "        for i, (x, y) in tqdm.tqdm(enumerate(train_loader)):\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            x, y = x.to(device), y.to(device, non_blocking=True)\n",
    "            logits, logqp = model(\n",
    "                x, dt=args.dt, adjoint=args.adjoint, method=args.method, adaptive=args.adaptive, adjoint_adaptive=args.adjoint_adaptive, rtol=args.rtol, atol=args.atol\n",
    "            )\n",
    "            nfes = model.nfe\n",
    "            xent = F.cross_entropy(logits, y, reduction=\"mean\")\n",
    "            loss = xent + args.kl_coeff * logqp\n",
    "            obj, kl = loss, args.kl_coeff * logqp\n",
    "            predictions = logits.detach().argmax(dim=1)\n",
    "            accuracy = torch.eq(predictions, y).float().mean()\n",
    "            train_ece = utils.score_model(logits.detach().cpu().numpy(), y.detach().cpu().numpy())[2]\n",
    "            ece.step(train_ece)\n",
    "            nfe.step(nfes)\n",
    "            loss.mean().backward()  # retain_graph=True\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_xent.step(loss.mean())\n",
    "            train_accuracy.step(accuracy)\n",
    "            utils.ema_update(model=model, ema_model=ema_model, gamma=args.gamma)\n",
    "            global_step += 1\n",
    "            itr_per_epoch += 1\n",
    "            gc.collect()\n",
    "            # per itr nfes: {global step: [train nfe, [for each pause_every:] val nfe, val nfe ema, test nfe,\n",
    "            # test nfe ema]}\n",
    "            info[\"nfes\"] = {global_step: [nfes]}\n",
    "\n",
    "            if global_step % args.pause_every == 0:\n",
    "                # tb_writer.add_scalar(f'Grad Norm (pause/{args.pause_every})', torch.norm(x.grad), global_step)\n",
    "                # TODO: magnitude of learned drift function\n",
    "                # drift_y = model.f(0, aug_y)[:y.numel()]\n",
    "                add_scalar_(f'Activation Norm (pause/{args.pause_every})', torch.norm(y.detach().cpu().float()).numpy().mean(), global_step)\n",
    "                add_scalar_(f'NFE/train (pause/{args.pause_every})', nfes, global_step)\n",
    "                val_xent, val_accuracy, val_ece, val_nfe = evaluate(model, validate=True)\n",
    "                val_xent_ema, val_accuracy_ema, val_ece_ema, val_nfe_ema = evaluate(ema_model, validate=True)\n",
    "                info['nfes'][global_step].extend([val_nfe, val_nfe_ema])\n",
    "                if val_accuracy > best_val_acc:\n",
    "                    best_val_acc = val_accuracy\n",
    "                    utils.save_ckpt(model, ema_model, optimizer, os.path.join(output_dir, \"best_val_acc.ckpt\"),\n",
    "                                    scheduler, epoch=epoch, global_step=global_step, best_acc=best_test_acc,\n",
    "                                    best_val=best_val_acc, info=info)\n",
    "                add_scalar_('Accuracy/val', val_accuracy, global_step)\n",
    "                add_scalar_('Accuracy EMA/val', val_accuracy_ema, global_step)\n",
    "                add_scalar_('NLL/val', val_xent, global_step)\n",
    "                add_scalar_('NLL EMA/val', val_xent_ema, global_step)\n",
    "                add_scalar_('ECE/val', val_ece, global_step)\n",
    "                add_scalar_('ECE EMA/val', val_ece_ema, global_step)\n",
    "                add_scalar_('NFE/val (total/inference)', val_nfe, global_step)\n",
    "                add_scalar_('NFE EMA/val (total/inference)', val_nfe_ema, global_step)\n",
    "                logging.warning(\n",
    "                    f\"global step: {global_step}, \"\n",
    "                    f\"epoch: {epoch}, \"\n",
    "                    f\"train_xent: {train_xent.val:.4f}, \"\n",
    "                    f\"train_accuracy: {train_accuracy.val:.4f}, \"\n",
    "                    f\"val_xent: {val_xent:.4f}, \"\n",
    "                    f\"val_accuracy: {val_accuracy:.4f}, \"\n",
    "                    f\"val_xent_ema: {val_xent_ema:.4f}, \"\n",
    "                    f\"val_accuracy_ema: {val_accuracy_ema:.4f}\"\n",
    "                )\n",
    "\n",
    "        epoch_time = epoch_start - time.time()\n",
    "        utils.save_ckpt(model, ema_model, optimizer, os.path.join(output_dir, \"state.ckpt\"), scheduler, epoch=epoch,\n",
    "                        global_step=global_step, best_val=best_val_acc, best_acc=best_test_acc, info=info)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        add_scalar_('Accuracy/train', train_accuracy.val, epoch)\n",
    "        add_scalar_('NLL/train', train_xent.val, epoch)\n",
    "        add_scalar_('KL/train', kl, epoch)\n",
    "        add_scalar_('Loss/train', obj, epoch)\n",
    "        add_scalar_('ECE/train', ece.val, epoch)\n",
    "        add_scalar_('NFE/train (avg/epoch)', nfe.val, epoch)\n",
    "        nfe.__init__()  # reset for new epoch\n",
    "        logging.warning(\"Wrote training scalars to tensorboard\")\n",
    "\n",
    "        test_xent, test_accuracy, test_ece, test_nfe = evaluate(model)\n",
    "        test_xent_ema, test_accuracy_ema, test_ece_ema, test_nfe_ema = evaluate(ema_model)\n",
    "        info['nfes'][global_step].extend([test_nfe, test_nfe_ema])\n",
    "        add_scalar_('Accuracy/test', test_accuracy, epoch)\n",
    "        add_scalar_('Accuracy EMA/test', test_accuracy_ema, epoch)\n",
    "        add_scalar_('NLL/test', test_xent, epoch)\n",
    "        add_scalar_('NLL EMA/test', test_xent_ema, global_step)\n",
    "        add_scalar_('ECE/test', test_ece, epoch)\n",
    "        add_scalar_('ECE EMA/test', test_ece_ema, epoch)\n",
    "        add_scalar_('NFE/test (total/inference)', test_nfe, epoch)\n",
    "        add_scalar_('NFE EMA/test (total/inference)', test_nfe_ema, epoch)\n",
    "        logging.warning(\"Wrote test scalars to tensorboard\")\n",
    "        if test_accuracy > best_test_acc:\n",
    "            best_test_acc = test_accuracy\n",
    "            utils.save_ckpt(model, ema_model, optimizer, os.path.join(output_dir, \"best_test_acc.ckpt\"), scheduler,\n",
    "                            epoch=epoch, global_step=global_step, best_val=best_val_acc, best_acc=best_test_acc,\n",
    "                            info=info)\n",
    "        with open(os.path.join(output_dir, \"results.txt\"), \"a\") as f:\n",
    "            f.write(f\"Epoch {epoch} (step {global_step}) in {epoch_time:.4f} sec | Train acc {train_accuracy.val}\" + \\\n",
    "                    f\" | Test accuracy {test_accuracy} | Test EMA accuracy {test_accuracy_ema}\" + \\\n",
    "                    f\" | Train NLL {train_xent.val} | Test NLL {test_xent} | Test EMA NLL {test_xent_ema} | Train \"\n",
    "                    f\"Loss \" + \\\n",
    "                    f\" {obj.detach().cpu().numpy().tolist()} | Train KL {kl}\" + \\\n",
    "                    f\" | Train ECE {ece.val} | Test ECE {test_ece} | Test ECE EMA {test_ece_ema}\" + \\\n",
    "                    f\" | Train nfes {nfe.val} | Test NFE {test_nfe} | Test NFE EMA {test_nfe_ema}\\n\")\n",
    "            logging.warning(f\"Wrote epoch info to {os.path.join(output_dir, 'results.txt')}\")\n",
    "        info[global_step] = {'epoch': epoch, 'time': epoch_time, 'train_acc': train_accuracy.val,\n",
    "                             'test_acc': test_accuracy,\n",
    "                             'train_nll': train_xent.val, 'test_nll': test_xent, 'test_ema_nll': test_xent_ema,\n",
    "                             'train_loss': obj.detach().cpu().numpy().tolist(),\n",
    "                             'train_kl': kl.detach().cpu().numpy().tolist(), \"val_acc\": val_accuracy,\n",
    "                             \"val_xent\": val_xent, \"val_xent_ema\": val_xent_ema,\n",
    "                             \"train_ece\": ece.val, \"test_ece\": test_ece, \"test_ece_ema\": test_ece_ema,\n",
    "                             \"itr_per_epoch\": itr_per_epoch, \"avg_train_nfe\": nfe.val,\n",
    "                             \"test_nfe\": test_nfe, \"test_nfe_ema\": test_nfe_ema}\n",
    "        utils.write_state_config(info, args.train_dir, file_name='state.json')\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _evaluate_with_loader(model, loader):\n",
    "    xents = []\n",
    "    accuracies = []\n",
    "    eces = []\n",
    "    nfes = 0\n",
    "    model.eval()\n",
    "    for i, (x, y) in enumerate(loader, 1):\n",
    "        x, y = x.to(device), y.to(device, non_blocking=True)\n",
    "        logits, _ = model(x, dt=args.dt, adjoint=args.adjoint, adjoint_adaptive=args.adjoint_adaptive, method=args.method)  # , rtol=args.rtol, atol=args.atol)\n",
    "        loss = F.cross_entropy(logits, y, reduction=\"none\")\n",
    "        predictions = logits.detach().argmax(dim=1)\n",
    "        accuracy = torch.eq(predictions, y).float()\n",
    "        scores = utils.score_model(logits.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "        xents.append(loss)\n",
    "        accuracies.append(accuracy)\n",
    "        eces.append(torch.tensor([scores[2]]))\n",
    "        nfes += model.nfe\n",
    "        if i >= args.eval_batches: break\n",
    "    return tuple(torch.cat(x, dim=0).mean(dim=0).cpu().item() for x in (xents, accuracies, eces)) + (nfes,)\n",
    "\n",
    "\n",
    "def evaluate(model, validate=False):\n",
    "    if validate:\n",
    "        logging.warning(\"evaluating on validation set\")\n",
    "        test_xent, test_accuracy, test_ece, test_nfe = _evaluate_with_loader(model, val_loader)\n",
    "    else:\n",
    "        logging.warning(\"evaluating on test set\")\n",
    "        test_xent, test_accuracy, test_ece, test_nfe = _evaluate_with_loader(model, test_loader)\n",
    "    return test_xent, test_accuracy, test_ece, test_nfe\n",
    "\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer,\n",
    "                                    num_training_steps,\n",
    "                                    num_warmup_steps=0,\n",
    "                                    num_cycles=7. / 16.,\n",
    "                                    last_epoch=-1):\n",
    "    def _lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        no_progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0., math.cos(math.pi * num_cycles * no_progress))\n",
    "\n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, _lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_chw = (3, 32, 32)\n",
    "    if args.data == \"mnist\":\n",
    "        input_chw = (1, 28, 28)\n",
    "    if args.model == \"baseline\":\n",
    "        model = models.BaselineYNet(\n",
    "            input_size=input_chw,\n",
    "            activation=args.activation,\n",
    "            hidden_width=args.hidden_width\n",
    "        )\n",
    "    elif args.model == \"sdebnn\":\n",
    "        model = SDENet(\n",
    "            input_size=input_chw,\n",
    "            inhomogeneous=args.inhomogeneous,\n",
    "            activation=args.activation,\n",
    "            verbose=args.verbose,\n",
    "            hidden_width=args.hidden_width,\n",
    "            weight_network_sizes=(1,128,1),\n",
    "            blocks=(2,2,2),\n",
    "            sigma=args.sigma,\n",
    "            aug_dim=args.aug,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {args.model}\")\n",
    "    ema_model = copy.deepcopy(model)\n",
    "    model.to(device)\n",
    "    ema_model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(lr=args.lr, params=model.parameters())\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_training_steps=args.epochs * (50000 // args.batch_size))\n",
    "\n",
    "    start_epoch, best_test_acc, best_val_acc, global_step = 0, 0, 0, 0\n",
    "    info = collections.defaultdict(dict)\n",
    "    if os.path.exists(os.path.join(args.train_dir, \"state.ckpt\")):\n",
    "        # if os.path.exists(os.path.join(args.train_dir, \"best_val_acc.ckpt\")): # TODO: for debugging\n",
    "        logging.warning(\"Loading checkpoints...\")\n",
    "        checkpoint = torch.load(os.path.join(args.train_dir, \"state.ckpt\"))\n",
    "        # checkpoint = torch.load(os.path.join(args.train_dir, \"best_val_acc.ckpt\")) # TODO: for debugging\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_test_acc = checkpoint['best_acc']\n",
    "        best_val_acc = checkpoint['best_val_acc']\n",
    "        info = checkpoint['info']\n",
    "        global_step = checkpoint['global_step']\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "        ema_model.load_state_dict(checkpoint[\"ema_model\"])\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        logging.warning(f\"Successfully loaded checkpoints for epoch {start_epoch} | best acc {best_test_acc}\")\n",
    "\n",
    "    logging.warning(f'model: {model}')\n",
    "    logging.warning(f'{utils.count_parameters(model) / 1e6:.4f} million parameters')\n",
    "\n",
    "    tb_writer = SummaryWriter(os.path.join(args.train_dir, 'tb'))\n",
    "    train(\n",
    "        model, ema_model, optimizer, scheduler, args.epochs,\n",
    "        output_dir=args.train_dir, global_step=global_step, start_epoch=start_epoch,\n",
    "        best_test=best_test_acc, best_val=best_val_acc, info=info, tb_writer=tb_writer\n",
    "    )\n",
    "    tb_writer.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-dir', type=str, default='train', required=False)\n",
    "    parser.add_argument('--seed', type=int, default=1000000)\n",
    "    parser.add_argument('--no-gpu', action=\"store_true\")\n",
    "    parser.add_argument('--subset', type=int, default=None, help=\"Use subset of mnist data.\")\n",
    "    parser.add_argument('--data', type=str, default=\"mnist\", choices=['mnist', 'cifar10', 'cifar100'])\n",
    "    parser.add_argument('--pin-memory', type=utils.str2bool, default=True)\n",
    "    parser.add_argument('--num-workers', type=int, default=1)\n",
    "    parser.add_argument('--model', type=str, choices=['baseline', 'sdebnn'], default='sdebnn')\n",
    "    parser.add_argument('--method', type=str, choices=['milstein', 'midpoint', \"heun\", \"euler_heun\"], default='midpoint')\n",
    "    parser.add_argument('--gamma', type=float, default=0.999)\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=1e-3)\n",
    "    parser.add_argument('--aug', type=int, default=0)\n",
    "    parser.add_argument('--epochs', type=int, default=2)\n",
    "    parser.add_argument('--batch-size', type=int, default=128)\n",
    "    parser.add_argument('--eval-batch-size', type=int, default=512)\n",
    "    parser.add_argument('--pause-every', type=int, default=200) \n",
    "    parser.add_argument('--eval-batches', type=int, default=10000)\n",
    "\n",
    "    # Model.\n",
    "    parser.add_argument('--dt', type=float, default=0.1)\n",
    "    parser.add_argument('--rtol', type=float, default=1e-5)\n",
    "    parser.add_argument('--atol', type=float, default=1e-4)\n",
    "    parser.add_argument('--steps', type=int, default=20)\n",
    "    parser.add_argument('--adjoint', type=utils.str2bool, default=False)\n",
    "    parser.add_argument('--adaptive', type=utils.str2bool, default=False)\n",
    "    parser.add_argument('--adjoint_adaptive', type=utils.str2bool, default=False)\n",
    "    parser.add_argument('--inhomogeneous', type=utils.str2bool, default=True)\n",
    "    parser.add_argument('--activation', type=str, default=\"softplus\",\n",
    "                        choices=['swish', 'mish', 'softplus', 'tanh', 'relu', 'elu'])\n",
    "    parser.add_argument('--verbose', type=utils.str2bool, default=False)\n",
    "    parser.add_argument('--hidden-width', type=int, default=32)\n",
    "    parser.add_argument('--fw-width', type=str, default=\"1-128-1\")\n",
    "    parser.add_argument('--nblocks', type=str, default=\"2-2-2\")\n",
    "    parser.add_argument('--sigma', type=float, default=0.1)\n",
    "\n",
    "    parser.add_argument('--momentum', type=float, default=0.9)\n",
    "    parser.add_argument('--nesterov', type=utils.str2bool, default=True)\n",
    "    parser.add_argument('--kl-coeff', type=float, default=1e-3, help='Coefficient on the KL term.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and not args.no_gpu else 'cpu')\n",
    "    torch.backends.cudnn.benchmark = True  # noqa\n",
    "\n",
    "    utils.manual_seed(args)\n",
    "    utils.write_config(args)\n",
    "\n",
    "    print(args.pin_memory, args.num_workers)\n",
    "\n",
    "    train_loader, val_loader, test_loader = utils.get_loader(\n",
    "        args.data,\n",
    "        train_batch_size=args.batch_size,\n",
    "        test_batch_size=args.eval_batch_size,\n",
    "        pin_memory=args.pin_memory,\n",
    "        num_workers=args.num_workers,\n",
    "        subset=args.subset,\n",
    "        task=\"classification\"\n",
    "    )\n",
    "\n",
    "    logging.warning(\n",
    "        f\"Training set size: {utils.count_examples(train_loader)}, \"\n",
    "        f\"Val set size: {utils.count_examples(val_loader)}, \"\n",
    "        f\"test set size: {utils.count_examples(test_loader)}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:model: SDENet(\n",
      "  (y_net): Sequential(\n",
      "    (0): ConcatConv2d(2, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): DiffEqWrapper(\n",
      "      (module): Softplus(beta=1, threshold=20)\n",
      "    )\n",
      "    (2): ConcatConv2d(33, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): DiffEqWrapper(\n",
      "      (module): Softplus(beta=1, threshold=20)\n",
      "    )\n",
      "    (4): ConcatConv2d(33, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): DiffEqWrapper(\n",
      "      (module): Softplus(beta=1, threshold=20)\n",
      "    )\n",
      "    (6): ConcatConv2d(2, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): DiffEqWrapper(\n",
      "      (module): Softplus(beta=1, threshold=20)\n",
      "    )\n",
      "    (8): ConcatConv2d(33, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): DiffEqWrapper(\n",
      "      (module): Softplus(beta=1, threshold=20)\n",
      "    )\n",
      "    (10): ConcatConv2d(33, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): DiffEqWrapper(\n",
      "      (module): Softplus(beta=1, threshold=20)\n",
      "    )\n",
      "    (12): ConvDownsample(2, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (13): ConcatConv2d(5, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): DiffEqWrapper(\n",
      "      (module): Softplus(beta=1, threshold=20)\n",
      "    )\n",
      "    (15): ConcatConv2d(33, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): DiffEqWrapper(\n",
      "      (module): Softplus(beta=1, threshold=20)\n",
      "    )\n",
      "    (17): ConcatConv2d(33, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): DiffEqWrapper(\n",
      "      (module): Softplus(beta=1, threshold=20)\n",
      "    )\n",
      "    (19): ConcatConv2d(5, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): DiffEqWrapper(\n",
      "      (module): Softplus(beta=1, threshold=20)\n",
      "    )\n",
      "    (21): ConcatConv2d(33, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): DiffEqWrapper(\n",
      "      (module): Softplus(beta=1, threshold=20)\n",
      "    )\n",
      "    (23): ConcatConv2d(33, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): DiffEqWrapper(\n",
      "      (module): Softplus(beta=1, threshold=20)\n",
      "    )\n",
      "    (25): ConvDownsample(5, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (26): ConcatConv2d(17, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): DiffEqWrapper(\n",
      "      (module): Softplus(beta=1, threshold=20)\n",
      "    )\n",
      "    (28): ConcatConv2d(33, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): DiffEqWrapper(\n",
      "      (module): Softplus(beta=1, threshold=20)\n",
      "    )\n",
      "    (30): ConcatConv2d(33, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): DiffEqWrapper(\n",
      "      (module): Softplus(beta=1, threshold=20)\n",
      "    )\n",
      "    (32): ConcatConv2d(17, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): DiffEqWrapper(\n",
      "      (module): Softplus(beta=1, threshold=20)\n",
      "    )\n",
      "    (34): ConcatConv2d(33, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): DiffEqWrapper(\n",
      "      (module): Softplus(beta=1, threshold=20)\n",
      "    )\n",
      "    (36): ConcatConv2d(33, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (w_net): ModuleList(\n",
      "    (0): Linear(in_features=84560, out_features=1, bias=True)\n",
      "    (1): DiffEqWrapper(\n",
      "      (module): Tanh()\n",
      "    )\n",
      "    (2): Linear(in_features=1, out_features=128, bias=True)\n",
      "    (3): DiffEqWrapper(\n",
      "      (module): Tanh()\n",
      "    )\n",
      "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (5): DiffEqWrapper(\n",
      "      (module): Tanh()\n",
      "    )\n",
      "    (6): Linear(in_features=1, out_features=84560, bias=True)\n",
      "  )\n",
      "  (projection): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=1024, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "WARNING:root:1.1527 million parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size ynet: (1, 28, 28)\n",
      "(1, 28, 28) forloop\n",
      "(1, 28, 28) forloop\n",
      "(4, 14, 14) convdownsample\n",
      "(4, 14, 14) forloop\n",
      "(4, 14, 14) forloop\n",
      "(16, 7, 7) convdownsample\n",
      "(16, 7, 7) forloop\n",
      "(16, 7, 7) forloop\n",
      "initial_params (84560): torch.Size([84560])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 184913]) ww, aug_y1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2916: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n",
      "1it [00:13, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 184913]) ww, aug_y1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2916: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n",
      "1it [00:24, 24.74s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 403\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    400\u001b[0m logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mutils\u001b[38;5;241m.\u001b[39mcount_parameters(model)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1e6\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m million parameters\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    402\u001b[0m tb_writer \u001b[38;5;241m=\u001b[39m SummaryWriter(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mtrain_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 403\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mema_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_test_acc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_val_acc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_writer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_writer\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m tb_writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[29], line 207\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, ema_model, optimizer, scheduler, epochs, global_step, output_dir, start_epoch, best_test, best_val, info, tb_writer)\u001b[0m\n\u001b[1;32m    205\u001b[0m ece\u001b[38;5;241m.\u001b[39mstep(train_ece)\n\u001b[1;32m    206\u001b[0m nfe\u001b[38;5;241m.\u001b[39mstep(nfes)\n\u001b[0;32m--> 207\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# retain_graph=True\u001b[39;00m\n\u001b[1;32m    208\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    209\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024.0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "131072/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_params (141776): torch.Size([141776])\n"
     ]
    }
   ],
   "source": [
    "model = SDENet(\n",
    "input_size=(1,28,28),\n",
    "inhomogeneous=args.inhomogeneous,\n",
    "activation=args.activation,\n",
    "verbose=args.verbose,\n",
    "hidden_width=args.hidden_width,\n",
    "weight_network_sizes=(1,128,1),\n",
    "blocks=(2,2,2),\n",
    "sigma=args.sigma,\n",
    "aug_dim=args.aug,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 7, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "184913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size ynet: (1, 28, 28)\n",
      "(1, 28, 28) forloop\n",
      "(1, 28, 28) forloop\n",
      "(4, 14, 14) convdownsample\n",
      "(4, 14, 14) forloop\n",
      "(4, 14, 14) forloop\n",
      "(16, 7, 7) convdownsample\n",
      "(16, 7, 7) forloop\n",
      "(16, 7, 7) forloop\n",
      "(16, 7, 7)\n",
      "torch.Size([998096])\n",
      "torch.Size([5, 16, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "inputsize = (5, 1, 28, 28)\n",
    "sample = torch.randn(inputsize)\n",
    "\n",
    "time_sample = torch.tensor([0])\n",
    "y, out1= make_y_net(inputsize[1:],\n",
    "               blocks=(2, 2, 2),\n",
    "               activation=\"softplus\",\n",
    "               verbose=False,\n",
    "               explicit_params=True,\n",
    "               hidden_width=128,\n",
    "               aug_dim=0)\n",
    "print(out1)\n",
    "ee, ff = utils.ravel_pytree(y.make_initial_params())\n",
    "print(ee.shape)\n",
    "fyyy = y(time_sample, sample.reshape((-1, 1, 28,28)), ff(ee.squeeze(0)))\n",
    "print(fyyy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.reshape((-1, 1, 28,28)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1024])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fyyy.flatten(start_dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 16, 8, 8])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fyyy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 784])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.flatten(start_dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee, ff = utils.ravel_pytree(y.make_initial_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1890512])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ee.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 16, 8, 8])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fyyy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0716, -0.1845, -0.1163,  ...,  0.0176, -0.0211, -0.0175])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size ynet: (3, 28, 28)\n",
      "(12, 14, 14)\n",
      "(48, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "blocks = (2,2,2)\n",
    "input_size = (3,28,28)\n",
    "aug_dim=0\n",
    "\n",
    "_input_size = (input_size[0] + aug_dim,) + input_size[1:]\n",
    "layers = []\n",
    "print(f\"input_size ynet: {_input_size}\")\n",
    "\n",
    "for i, num_blocks in enumerate(blocks, 1):\n",
    "\n",
    "    if i < len(blocks):\n",
    "        \n",
    "        _input_size = _input_size[0] * 4, _input_size[1] // 2, _input_size[2] // 2\n",
    "        print(_input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300.0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "235200/784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[2,1,189397]\n",
    "_, aug = sdeint\n",
    "[1, 12432423], [1,12432423]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
